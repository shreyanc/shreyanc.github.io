---
published: true
layout: post
author: Shreyan Chowdhury
authors: 'Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli'
summary: >-
  Stacked convolutional neural networks are used to predict next time step in
  the waveform
logo: /assets/img/apad_wav2vec.png
link: 'https://arxiv.org/abs/1904.05862'
is_post: true
parent: /apad.html
title: 'Wave2Vec: Unsupervised Pre-Training for Speech Recognition'
---
## Upshot
- Pre-training using stacked double network - one **encoder** network and one **context** network.

## Pointers
- Choosing normalization scheme is important. They normalize both across the feature and temporal dimension for each sample. The layers in both the encoder and context networks consist of causal convolution with 512 channels, a group normalization layter and a ReLU nonlinearity.

## Details
- For decoding the emissions from the acoustic model, they use a lexicon and a separate language model. The word sequence <span>$\mathbf{y}$</span> is decoded from the context vector <span>$\mathbf{c}$</span> using the **beam search decoder**. 
- Objective: We train the model to distinguish a sample <span>$z_{i+k}$</span> that is k steps in the future from distractor samples <span>$\tilde z$</span> drawn from a proposal distribution <span>$p_n$</span>, by minimizing the contrastive loss for each step <span>$k = 1, ... ,K:$</span>

